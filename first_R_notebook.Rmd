---
title: "R Notebook"
output: html_notebook
---

# install packages
```{r}
#install.packages("tidyverse",dependencies = TRUE)
#install.packages("ggthemes",dependencies = TRUE)
#install.packages("rstatix", dependencies = TRUE)
#install.packages("ggpubr", dependencies = TRUE)
#install.packages("emmeans", dependencies = TRUE)
```

# load packages
```{r}
library(tidyverse)
library(ggthemes)
library(rstatix)
library(ggpubr)
library(emmeans)
```

# import data
```{r}
exp1 = read_csv("exp1.csv")

nrow(exp1)
ncol(exp1)
colnames(exp1)
hist(exp1$rt)
```
# tidyverse verbs
```{r}

condition_data = exp1 %>% 
  select(subject, rt, typeoftrial, trialcondition, correct) %>%
  filter(typeoftrial=="picture" &
           trialcondition %in%
           c("Heavy_Normal","Light_Smashed","Heavy_Smashed","Light_Normal"))

condition_data = condition_data %>%
  separate(trialcondition, into = c("weight", "shape"), sep = "_")


means = condition_data %>%
  group_by(weight, shape) %>%
  summarise(mean_rt = mean(rt),
    sd_rt = sd(rt),
    mean_acc = mean(correct))

means %>%
  ggplot(aes(x = shape, y = sd_rt, fill = weight)) +
  geom_col(position = "dodge") +
  labs(x = "Shape", y = "RT", title = "Barplot of RT")
```
# exercise
```{r}
exp1 %>%
  select(dataset, correct, typeoftrial) %>%
  filter(correct==1 & typeoftrial=="picture")
```
# ggplot exercise
```{r}
library(ggplot2)
library(ggthemes)
condition_data %>%
  filter(rt<25000) %>%
  ggplot(aes(x=rt)) +
  geom_histogram(binwidth = 1000,
                 fill = "purple", 
                 color = "black",
                 position = "identity") +
  theme_few() + 
  xlab("RT (in milliseconds)") +
  ylab("Count") +
  ggtitle("Histogram of RT")

condition_data %>%
  group_by(subject, trialcondition) %>%
  count()

```
# class exercises
```{r}
df = read_csv("relatedness_exp.csv")

part_df = df %>%
  select(ID, Type, pathlength, RT) %>%
  filter(Type=="Related" &
           pathlength %in% c("1","2"))
          #could also do (pathlength==1 | pathlength==2)

mean_all = df %>%
  summarise(means = mean(RT))


mean_part = part_df %>%
  summarise(mean = mean(RT))

mean_ID = df %>%
  group_by(ID) %>%
  summarise(mean = mean(RT))

#nrow(mean_ID) would tell you how many rows there are, do in console
#df %>% count() or nrow(df)

df %>%
group_by(ID, pathlength) %>% 
  count()

mean_condition = df %>%
  group_by(Type, pathlength)%>%
  summarise(mean_rt = mean(RT))

mean_condition %>%
  ggplot(aes(x = pathlength, y = mean_rt)) +
  aes(fill = Type) +
  geom_col(position = "dodge") +
  theme_few() + 
  xlab("Path length") +
  ylab("Mean RT") +
  ggtitle ("Barplot of Mean RT")

```
```{r}
df = df %>%
  mutate(RT_seconds == RT/1000, accuracy = ifelse(pathlength==1 & Type == "Related", 1, 0))

df = df %>%
  mutate(pathlength = as.factor(pathlength), 
         Type = as.factor(Type))

df %>% pull (Type)
df %>% pull (RT)

levels (df %>% pull (pathlength))
#answers how may levels does the variable pathlength have?
#can only see if you do mutate and make it a factor

df %>% pull (ID) %>% unique()
unique(c(1,3,3,4,5,5,1,2))
#omits all repetitions

```
# Wide data conversion
```{r}
df_wide1 = mean_condition %>%
  pivot_wider(names_from = pathlength, values_from = mean_rt)

df_wide2 = mean_condition %>%
  pivot_wider(names_from = Type, values_from = mean_rt)

exp1_wide = means %>%
  select(weight, shape, mean_rt) %>%
  # selects only these columns to deal with
  pivot_wider(names_from = weight, values_from = mean_rt)

mean_subject = condition_data %>%
  group_by(subject, shape, weight) %>%
  summarise(mean_rt = mean(rt))

subject_wide = mean_subject %>%
  pivot_wider(names_from = c(weight, shape), values_from = mean_rt)

```
# T-test
```{r}
t.test(subject_wide$Heavy_Normal, subject_wide$Heavy_Smashed, 
       var.equal = TRUE, paired = TRUE)

mean_subject %>%
  filter(weight=="Heavy") %>% 
  t.test(mean_rt ~ shape, data =.)
  #does t test vary as a function of shape?
  #paired t test because both come samples come from subject
```
# Verifying R output
```{r}
x1_bar = mean(subject_wide$Heavy_Normal)
x2_bar = subject_wide %>% pull(Heavy_Smashed) %>% mean()

s1 = sd(subject_wide$Heavy_Normal)
s2 = subject_wide %>% pull(Heavy_Smashed) %>% sd()

n1 = nrow(subject_wide)
n2 = subject_wide %>% pull(Heavy_Smashed) %>% length()

t_numerator = x1_bar - x2_bar
inside_root1 = s1*s1/n1
inside_root2 = s2*s2/n1
t_denominator = sqrt(inside_root1 + inside_root2)
t = t_numerator/t_denominator
```
# two tailed t-test
```{r}
x = s1*s1/n1
y = s2*s2/n2
a = s1*s1*s1*s1/(n1*n1*(n1-1))
b = s2*s2*s2*s2/(n2*n2*(n2-1))

deg = (x+y)*(x+y)/(a+b)

p_value = 2*(1-pt(t,deg))
```
# assumptions
```{r}
library(rstatix)
library(ggpubr)

# before you do t-test, need to check assumptions!!
# it proves a p value that indicates the probability of obtainign the difference you obtained or larger if the null hypothesis was true

subject_wide = subject_wide %>%
  mutate(diff = Heavy_Normal - Heavy_Smashed)

subject_wide = subject_wide %>% ungroup()
# for some reason rstatix always groups, so need to ungroup

outliers = subject_wide %>% identify_outliers(diff)

outlier_subs = outliers %>% pull(subject)
# taking just the subjects in the outliers

subject_wide_new = subject_wide %>% filter(!subject %in% outlier_subs)
# filtering out outlier subjects from the data

hist(subject_wide$diff)

ggqqplot(subject_wide, "diff")
#running qq plot to see what it is vs what you expect, running through origin

subject_wide %>% shapiro_test(diff)
# want to find NON-significant result (p > 0.05)
# a formal test

hist(subject_wide_new$"diff")
ggqqplot(subject_wide_new, "diff")
# after taking out 12 outliers, it's much better

subject_wide_new %>% shapiro_test(diff)
#now we can do t-test because it's non-significant

#t-test
t.test(subject_wide_new$Heavy_Normal, subject_wide_new$Heavy_Smashed, 
       var.equal = TRUE, paired = TRUE)
  # so now we know that even without the outliers, it's still not significant
```
# assumptions for exercise
```{r}
subject_wide = subject_wide %>%
  mutate(diff2 = Light_Normal - Heavy_Normal)

subject_wide = subject_wide %>% ungroup()

outliers2 = subject_wide %>% identify_outliers(diff2)

new_df = outliers2 %>% pull(subject)

subject_wide_new2 = subject_wide %>% filter(!subject %in% new_df)

hist(subject_wide$diff2)

ggqqplot(subject_wide, "diff2")

subject_wide %>% shapiro_test(diff2)
# have p value that's high enough, so can do t-test

hist(subject_wide_new2$"diff2")
ggqqplot(subject_wide_new2, "diff2")

subject_wide_new2 %>% shapiro_test(diff2)
# have p value that's high enough, so can do t-test

t.test(subject_wide_new2$Light_Normal, subject_wide_new2$Heavy_Normal, 
       var.equal = TRUE, paired = TRUE)
  # also no difference because p > 0.05 even after excluding outliers
```
# back to long data
```{r}
subject_long = subject_wide %>%
  select(-c(diff, diff2)) %>%
  pivot_longer(names_to = "condition", cols = Heavy_Normal:Light_Smashed)

new_means = means %>%
  select(weight, shape, mean_rt)

new_means_wide = new_means %>%
  pivot_wider(names_from = c(weight, shape), values_from = mean_rt)

new_means_long = new_means_wide %>%
  pivot_longer(names_to = "condition", cols = Heavy_Normal:Light_Smashed)

new_means_long = new_means_long %>%
  separate(condition, into = c("weight","shape")) %>%
  rename(mean_rt = value)
```
# ANOVA one-way
```{r}
data("iris")
View(iris)

levels(iris$Species)

iris %>%
  ggplot(aes(x = Species, y = Petal.Length)) + geom_boxplot()
#there is a difference so can do t test individually - would be 3

iris %>%
  group_by(Species) %>%
  identify_outliers(Petal.Length)

iris %>% 
  group_by(Species) %>%
  shapiro_test(Petal.Length)

ggqqplot(iris, "Petal.Length", facet.by = "Species")

iris %>%
  group_by(Species) %>%
  summarise(sd = sd(Petal.Length), var = var(Petal.Length))

iris %>% levene_test(Petal.Length ~ Species)

# if all assumptions are met, this is a regular anova, but homogeneity was violated so:
iris %>% anova_test(Petal.Length ~ Species)

#so use welch test, corrects for different variances
iris %>% welch_anova_test(Petal.Length ~ Species)
# high F: difference between groups is higher than difference within groups
# low p-value, so reject null; this values is really unlikely if between group variance was caused by chance. there is a between group difference

#POST HOC test
pwc = iris %>% tukey_hsd(Petal.Length ~ Species)

pwc2 <- iris %>% games_howell_test(Petal.Length ~ Species)

```
# ANOVA 2 way
```{r}
install.packages("datarium")

data("jobsatisfaction", package = "datarium")
View(jobsatisfaction)

jobsatisfaction %>%
  group_by(gender, education_level) %>%
  summarise(mean = mean(score), 
            sd = sd(score))

jobsatisfaction %>% 
  ggplot(aes(x = gender, y = score, color = education_level)) + 
  geom_boxplot()

# now check assumptions: outliers, normality, homogeneity

jobsatisfaction %>%
  group_by(gender, education_level) %>%
  shapiro_test(score)
  # p > 0.05, so high enough, yes distribution is significantly different

jobsatisfaction %>%
  levene_test(score ~ gender*education_level)
  # satisfied based on dfs?

jobsatisfaction %>%
  anova_test(score ~ gender*education_level)

# find simple effects because we found significant effects
model <-lm(score ~ gender * education_level, data = jobsatisfaction)
# to get right degrees of freedom for anova test

jobsatisfaction %>%
  group_by(gender) %>%
  anova_test(score ~ education_level, error = model)

# use emmeans bc found simple effects:
jobsatisfaction %>%
  group_by(gender) %>%
  emmeans_test(score ~ education_level, p.adjust.method = "bonferroni")

# what about a non-significant interaction? then we evaluate the main effects and see which produced significant. then you decompose main effects
# only for 2 levels for independent variables
jobsatisfaction %>%
  pairwise_t_test(score ~ education_level, p.adjust.method = "bonferroni")

```
# repeated measures anova
```{r}
relatedness = read_csv("relatedness.csv")

# levels of path length
relatedness = relatedness %>%
  mutate(pathlength = as.factor(pathlength))

levels(relatedness$pathlength)
# trials per participant
relatedness %>%
  group_by(ID) %>%
  count()
#trials per path length
relatedness %>%
  group_by(ID, pathlength) %>%
  count()

# Mean time taken to make relatness judgemets as a functino of type of judgment and path length
mean_rt = relatedness %>%
  group_by(Type, pathlength) %>%
  summarise(rt = mean(RT))
#barplot
mean_rt %>%
  ggplot(aes(x = pathlength, y = rt, group = Type, fill = Type)) + 
  geom_col(position = "dodge")+
  theme_clean()+
  scale_fill_wsj()


subject_rt = relatedness %>%
  group_by(ID, pathlength, Type) %>%
  summarise(rt = mean(RT)) %>%
  mutate(logRT = log(rt))

# histogram
hist(subject_rt$rt)
hist(subject_rt$logRT)

#check outliers
outliers = subject_rt %>%
  group_by(pathlength, Type) %>%
  identify_outliers(logRT)
#check normality

subject_rt = subject_rt %>% ungroup()

subject_rt %>%
  group_by(pathlength, Type) %>%
  shapiro_test(logRT)

ggqqplot(subject_rt, "logRT", ggtheme = theme_bw()) +
  facet_grid(pathlength ~ Type, labeller = "label_both")

anova_test(data = subject_rt, dv = logRT, wid = ID, within = c(pathlength, Type))

rm.aov = anova_test(
  data = subject_rt, dv = logRT, wid = ID, within = c(pathlength, Type))

get_anova_table(rm.aov) # there are significant interactions!

#let's look at the effect of Type at each level of path length

subject_rt %>%
  group_by(Type) %>%
  anova_test(dv = logRT, wid = ID, within = pathlength) %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
# there are simple effects so need to look at pairwise t tests

pwt = subject_rt %>%
  group_by(Type) %>%
  pairwise_t_test(logRT ~ pathlength, paired = TRUE, p.adjust.method = "bonferroni")
```






